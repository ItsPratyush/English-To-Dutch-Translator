{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "h5zNIsUkAffK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and prepare the dataset\n",
        "\n",
        "!wget https://www.statmt.org/europarl/v7/nl-en.tgz\n",
        "!tar -xzf nl-en.tgz\n",
        "\n",
        "en_path = \"europarl-v7.nl-en.en\"\n",
        "nl_path = \"europarl-v7.nl-en.nl\"\n",
        "\n",
        "max_samples = 50000\n",
        "\n",
        "def read_lines(file_path, max_lines):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = [line.strip() for i, line in enumerate(file) if i < max_lines]\n",
        "    return lines\n",
        "\n",
        "english_lines = read_lines(en_path, max_samples)\n",
        "dutch_lines = read_lines(nl_path, max_samples)\n",
        "\n",
        "print(f\"Loaded {len(english_lines)} English sentences\")\n",
        "print(f\"Loaded {len(dutch_lines)} Dutch sentences\")\n",
        "print(\"\\nExample pairs:\")\n",
        "for i in range(3):\n",
        "    print(f\"English: {english_lines[i]}\")\n",
        "    print(f\"Dutch: {dutch_lines[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7lW30SPA_5e",
        "outputId": "fcd91f48-6adc-4adc-f7e0-c31e033e6782"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-06 16:35:31--  https://www.statmt.org/europarl/v7/nl-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.32.28\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 199077856 (190M) [application/x-gzip]\n",
            "Saving to: ‘nl-en.tgz.3’\n",
            "\n",
            "nl-en.tgz.3         100%[===================>] 189.85M  20.9MB/s    in 11s     \n",
            "\n",
            "2025-07-06 16:35:43 (17.9 MB/s) - ‘nl-en.tgz.3’ saved [199077856/199077856]\n",
            "\n",
            "Loaded 50000 English sentences\n",
            "Loaded 50000 Dutch sentences\n",
            "\n",
            "Example pairs:\n",
            "English: Resumption of the session\n",
            "Dutch: Hervatting van de zitting\n",
            "\n",
            "English: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "Dutch: Ik verklaar de zitting van het Europees Parlement, die op vrijdag 17 december werd onderbroken, te zijn hervat. Ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad.\n",
            "\n",
            "English: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
            "Dutch: Zoals u heeft kunnen constateren, is de grote \"millenniumbug\" uitgebleven. De burgers van een aantal van onze lidstaten zijn daarentegen door verschrikkelijke natuurrampen getroffen.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join(text.split())\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "    return text\n",
        "\n",
        "english_lines = [preprocess_text(line) for line in english_lines]\n",
        "dutch_lines = [preprocess_text(line) for line in dutch_lines]\n",
        "\n",
        "train_en, test_en, train_nl, test_nl = train_test_split(\n",
        "    english_lines, dutch_lines, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_en)}\")\n",
        "print(f\"Test samples: {len(test_en)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryzKKI3aBTbp",
        "outputId": "bf178025-10ac-41b4-86eb-3895e28f4c26"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 40000\n",
            "Test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text vectorization (creating vocabularies)\n",
        "\n",
        "def build_vectorizer(lines, max_vocab_size=20000):\n",
        "    vectorizer = TextVectorization(\n",
        "        max_tokens=max_vocab_size,\n",
        "        standardize=None,\n",
        "        split='whitespace',\n",
        "        output_mode='int',\n",
        "        output_sequence_length=None\n",
        "    )\n",
        "    vectorizer.adapt(lines)\n",
        "    return vectorizer\n",
        "\n",
        "en_vectorizer = build_vectorizer(train_en)\n",
        "en_vocab = en_vectorizer.get_vocabulary()\n",
        "en_vocab_size = len(en_vocab)\n",
        "print(f\"English vocabulary size: {en_vocab_size}\")\n",
        "\n",
        "nl_vectorizer = build_vectorizer(train_nl)\n",
        "nl_vocab = nl_vectorizer.get_vocabulary()\n",
        "nl_vocab_size = len(nl_vocab)\n",
        "print(f\"Dutch vocabulary size: {nl_vocab_size}\")\n",
        "\n",
        "en_word2idx = {word: idx for idx, word in enumerate(en_vocab)}\n",
        "nl_word2idx = {word: idx for idx, word in enumerate(nl_vocab)}\n",
        "\n",
        "en_idx2word = {idx: word for idx, word in enumerate(en_vocab)}\n",
        "nl_idx2word = {idx: word for idx, word in enumerate(nl_vocab)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ID-f9LABstZ",
        "outputId": "8003a29a-9d8a-4e8f-9ed8-0e7900359730"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 20000\n",
            "Dutch vocabulary size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets\n",
        "\n",
        "def prepare_dataset(encoder_inputs, decoder_inputs, batch_size=16):\n",
        "    encoder_inputs = en_vectorizer(encoder_inputs)\n",
        "    decoder_inputs = nl_vectorizer(decoder_inputs)\n",
        "\n",
        "    decoder_targets = decoder_inputs[:, 1:]\n",
        "    decoder_inputs = decoder_inputs[:, :-1]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        ((encoder_inputs, decoder_inputs), decoder_targets))\n",
        "    dataset = dataset.shuffle(2048).batch(batch_size).prefetch(16)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = prepare_dataset(train_en, train_nl)\n",
        "val_dataset = prepare_dataset(test_en, test_nl)\n",
        "\n",
        "for (encoder_inputs, decoder_inputs), decoder_targets in train_dataset.take(1):\n",
        "    print(\"Encoder inputs shape:\", encoder_inputs.shape)\n",
        "    print(\"Decoder inputs shape:\", decoder_inputs.shape)\n",
        "    print(\"Decoder targets shape:\", decoder_targets.shape)\n",
        "\n",
        "    print(\"\\nFirst example:\")\n",
        "    print(\"English:\", \" \".join([en_idx2word[i] for i in encoder_inputs[0].numpy() if i != 0]))\n",
        "    print(\"Dutch input:\", \" \".join([nl_idx2word[i] for i in decoder_inputs[0].numpy() if i != 0]))\n",
        "    print(\"Dutch target:\", \" \".join([nl_idx2word[i] for i in decoder_targets[0].numpy() if i != 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5XnJR5xB2xm",
        "outputId": "c566dd6d-c972-4a7b-f595-b1f8747cf126"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder inputs shape: (16, 165)\n",
            "Decoder inputs shape: (16, 153)\n",
            "Decoder targets shape: (16, 153)\n",
            "\n",
            "First example:\n",
            "English: <start> berger report [UNK] <end>\n",
            "Dutch input: <start> verslagberger a500072000 <end>\n",
            "Dutch target: verslagberger a500072000 <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Encoder\n",
        "\n",
        "class Encoder(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        return output, state_h, state_c\n",
        "\n",
        "embedding_dim = 256\n",
        "hidden_units = 512\n",
        "\n",
        "encoder = Encoder(en_vocab_size, embedding_dim, hidden_units)\n",
        "\n",
        "sample_encoder_output, sample_encoder_state_h, sample_encoder_state_c = encoder(encoder_inputs)\n",
        "print(\"Encoder output shape:\", sample_encoder_output.shape)\n",
        "print(\"Encoder state_h shape:\", sample_encoder_state_h.shape)\n",
        "print(\"Encoder state_c shape:\", sample_encoder_state_c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmDKHnxsB_7b",
        "outputId": "fe8c5df9-1f91-4cd9-d052-b97bbed25b26"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (16, 165, 512)\n",
            "Encoder state_h shape: (16, 512)\n",
            "Encoder state_c shape: (16, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder with Bahdanau Attention\n",
        "\n",
        "class Decoder(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "\n",
        "        self.W1 = Dense(hidden_units)\n",
        "        self.W2 = Dense(hidden_units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "        self.dense = Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "\n",
        "        encoder_output, state_h, state_c = initial_state\n",
        "\n",
        "        x = self.embedding(inputs)  # shape: (batch_size, 1, embedding_dim)\n",
        "\n",
        "        lstm_output, state_h, state_c = self.lstm(x, initial_state=[state_h, state_c])\n",
        "\n",
        "        decoder_output_with_attention = tf.repeat(lstm_output, repeats=encoder_output.shape[1], axis=1)\n",
        "\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(encoder_output) + self.W2(decoder_output_with_attention)\n",
        "        ))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1, keepdims=True)\n",
        "\n",
        "        attention_output = tf.concat([context_vector, lstm_output], axis=-1)\n",
        "\n",
        "        output = self.dense(attention_output)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "decoder = Decoder(nl_vocab_size, embedding_dim, hidden_units)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(\n",
        "    tf.random.uniform((16, 1), maxval=nl_vocab_size, dtype=tf.int64),\n",
        "    initial_state=[sample_encoder_output, sample_encoder_state_h, sample_encoder_state_c])\n",
        "print(\"Decoder output shape:\", sample_decoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuQKR_n-CP8m",
        "outputId": "9920ede4-1fc7-4ae6-f3d6-cced810abfde"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (16, 1, 20000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the complete NMT model\n",
        "\n",
        "class NMTModel(Model):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(NMTModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_input, decoder_input = inputs\n",
        "\n",
        "        encoder_output, state_h, state_c = self.encoder(encoder_input)\n",
        "        decoder_state = [encoder_output, state_h, state_c]\n",
        "\n",
        "        batch_size = tf.shape(encoder_input)[0]\n",
        "        max_length = tf.shape(decoder_input)[1]\n",
        "\n",
        "        outputs = tf.TensorArray(tf.float32, size=max_length)\n",
        "\n",
        "        t0 = tf.constant(0)\n",
        "        cond = lambda t, *_: t < max_length\n",
        "\n",
        "        def body(t, outputs, state_h, state_c):\n",
        "            current_input = decoder_input[:, t:t+1]\n",
        "\n",
        "            output, state_h, state_c = self.decoder(\n",
        "                current_input, [encoder_output, state_h, state_c])\n",
        "\n",
        "            outputs = outputs.write(t, tf.squeeze(output, axis=1))\n",
        "            return t + 1, outputs, state_h, state_c\n",
        "\n",
        "        _, outputs, _, _ = tf.while_loop(\n",
        "            cond, body, loop_vars=[t0, outputs, state_h, state_c],\n",
        "            parallel_iterations=32)\n",
        "\n",
        "        outputs = outputs.stack()\n",
        "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
        "        return outputs\n",
        "\n",
        "\n",
        "model = NMTModel(encoder, decoder)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "sample_output = model((encoder_inputs, decoder_inputs))\n",
        "print(\"Model output shape:\", sample_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7DzBnw2Em6i",
        "outputId": "23dd4d17-2d1d-4255-f727-a433015f4d04"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output shape: (16, 153, 20000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"nmt_model.keras\", save_best_only=True)\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "    patience=5, restore_best_weights=True)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtEp76IxNZ4C",
        "outputId": "d97c42da-6537-4054-d9e4-441f217c7ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1074/2500\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:45:22\u001b[0m 4s/step - accuracy: 0.8388 - loss: 1.4884"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (Translation) functions\n",
        "\n",
        "class Translator:\n",
        "    def __init__(self, encoder, decoder, en_vectorizer, nl_vectorizer, nl_idx2word, nl_word2idx):\n",
        "        \"\"\"\n",
        "        Translator class for inference on trained NMT model.\n",
        "\n",
        "        Args:\n",
        "            encoder: Trained encoder model.\n",
        "            decoder: Trained decoder model.\n",
        "            en_vectorizer: TextVectorization for English.\n",
        "            nl_vectorizer: TextVectorization for Dutch.\n",
        "            nl_idx2word: Dictionary mapping Dutch token IDs to words.\n",
        "            nl_word2idx: Dictionary mapping Dutch words to token IDs.\n",
        "        \"\"\"\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.en_vectorizer = en_vectorizer\n",
        "        self.nl_vectorizer = nl_vectorizer\n",
        "        self.nl_idx2word = nl_idx2word\n",
        "        self.nl_word2idx = nl_word2idx\n",
        "        self.nl_vocab_size = len(nl_idx2word)\n",
        "\n",
        "    def translate(self, input_sentence, max_length=50):\n",
        "        \"\"\"\n",
        "        Translate a single English sentence to Dutch.\n",
        "\n",
        "        Args:\n",
        "            input_sentence (str): English sentence.\n",
        "            max_length (int): Max decoding steps to prevent infinite loops.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated Dutch translation.\n",
        "        \"\"\"\n",
        "        input_sentence = preprocess_text(input_sentence)\n",
        "        encoder_input = self.en_vectorizer([input_sentence])  # Shape: (1, seq_len)\n",
        "\n",
        "        encoder_output, state_h, state_c = self.encoder(encoder_input)\n",
        "        decoder_state = [encoder_output, state_h, state_c]\n",
        "\n",
        "        decoder_input = tf.expand_dims([self.nl_word2idx['<start>']], 0)  # Shape: (1, 1)\n",
        "\n",
        "        decoded_sentence = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            output, state_h, state_c = self.decoder(decoder_input, decoder_state)\n",
        "            decoder_state = [encoder_output, state_h, state_c]\n",
        "\n",
        "            predicted_id = tf.argmax(output, axis=-1).numpy()[0, 0].item()\n",
        "            predicted_word = self.nl_idx2word.get(predicted_id, '<unk>')\n",
        "\n",
        "            if predicted_word == '<end>':\n",
        "                break\n",
        "\n",
        "            decoded_sentence.append(predicted_word)\n",
        "            decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        return ' '.join(decoded_sentence)"
      ],
      "metadata": {
        "id": "fgKlDU7CjW6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(encoder, decoder, en_vectorizer, nl_vectorizer, nl_idx2word, nl_word2idx)\n",
        "import inspect\n",
        "print(inspect.getsource(translator.translate))"
      ],
      "metadata": {
        "id": "7B8zbjD2jr3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with BLEU score\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_translations(translator, test_sentences, reference_translations, n=100):\n",
        "    \"\"\"\n",
        "    Evaluate model translations on a subset of test data using BLEU.\n",
        "\n",
        "    Args:\n",
        "        translator: Translator object with translate() method.\n",
        "        test_sentences (list of str): English sentences to translate.\n",
        "        reference_translations (list of str): Corresponding Dutch references.\n",
        "        n (int): Number of samples to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        float: Average BLEU score.\n",
        "    \"\"\"\n",
        "    if n < len(test_sentences):\n",
        "        np.random.seed(42)\n",
        "        indices = np.random.choice(len(test_sentences), n, replace=False)\n",
        "        test_sentences = [test_sentences[i] for i in indices]\n",
        "        reference_translations = [reference_translations[i] for i in indices]\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i, (src, ref) in enumerate(zip(test_sentences, reference_translations)):\n",
        "\n",
        "        translation = translator.translate(src)\n",
        "\n",
        "        ref = ref.replace('<start>', '').replace('<end>', '').strip()\n",
        "        ref_tokens = ref.split()\n",
        "\n",
        "        trans_tokens = translation.split()\n",
        "\n",
        "        score = sentence_bleu([ref_tokens], trans_tokens, smoothing_function=smoothie)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Evaluated {i+1}/{n} sentences\", end='\\r')\n",
        "\n",
        "    avg_bleu = np.mean(bleu_scores)\n",
        "    print(f\"\\nAverage BLEU score over {n} samples: {avg_bleu:.4f}\")\n",
        "    return avg_bleu\n",
        "\n",
        "bleu_score = evaluate_translations(translator, test_en, test_nl)"
      ],
      "metadata": {
        "id": "riU7M5kehOcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "\n",
        "def interactive_translation(translator):\n",
        "    print(\"English to Dutch Translator (type 'quit' to exit)\")\n",
        "    while True:\n",
        "        sentence = input(\"Enter English sentence: \")\n",
        "        if sentence.lower() == 'quit':\n",
        "            break\n",
        "        translation = translator.translate(sentence)\n",
        "        print(f\"Translation: {translation}\\n\")\n",
        "\n",
        "interactive_translation(translator)"
      ],
      "metadata": {
        "id": "i0Bzh6_xh7oy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}